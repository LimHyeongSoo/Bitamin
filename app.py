import streamlit as st
import toml
import json
import os
import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import matplotlib.pyplot as plt


# ì„¤ì • íŒŒì¼ ë¡œë“œ
def load_config(config_path='config.toml'):
    with open(config_path, 'r', encoding='utf-8') as f:
        config = toml.load(f)
    return config


# ë²ˆì—­ íŒŒì¼ ë¡œë“œ
def load_translations(lang='ko', translations_dir='translations'):
    lang_file = os.path.join(translations_dir, f"{lang}.json")
    if not os.path.exists(lang_file):
        lang_file = os.path.join(translations_dir, "en-US.json")  # ê¸°ë³¸ ì–¸ì–´ ì„¤ì •
    with open(lang_file, 'r', encoding='utf-8') as f:
        translations = json.load(f)
    return translations


# ë‹¤êµ­ì–´ ì§€ì›ì„ ìœ„í•œ í•¨ìˆ˜
def translate(key, translations):
    keys = key.split('.')
    value = translations
    try:
        for k in keys:
            value = value[k]
        if isinstance(value, str):
            return value
        else:
            return key  # í‚¤ê°€ ë¬¸ìì—´ì´ ì•„ë‹ ê²½ìš° í‚¤ ìì²´ ë°˜í™˜
    except KeyError:
        return key  # í‚¤ê°€ ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš° í‚¤ ìì²´ ë°˜í™˜


# ì„¤ì • ë° ë²ˆì—­ íŒŒì¼ ë¡œë“œ
config = load_config()

# ì´ˆê¸°ê°’ ì„¤ì •
if "language" not in st.session_state:
    st.session_state.language = config.get('meta', {}).get('language', 'ko')  # ê¸°ë³¸ ì–¸ì–´ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥

translations = load_translations(lang=st.session_state.language)

# ì‚¬ìš©ì ì •ì˜ CSS ì ìš©
custom_css_path = config.get('UI', {}).get('custom_css', '')
if custom_css_path and os.path.exists(custom_css_path):
    with open(custom_css_path) as f:
        st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)

# CSSë¡œ ì¤‘ì•™ ì •ë ¬ ë° ìƒ‰ìƒ ì„¤ì •
st.markdown(
    """
    <style>
    .center-title {
        text-align: center;
        color: #FFA07A; /* Light Salmon: ì—°í•œ ì£¼í™©ìƒ‰ */
        font-size: 2.5em; /* ì œëª© í¬ê¸° ì¡°ì • */
        font-weight: bold;
    }
    .center-text {
        text-align: center;
        color: #FFA07A; /* Light Salmon: ì—°í•œ ì£¼í™©ìƒ‰ */
        font-size: 1.8em; /* ë¬¸êµ¬ í¬ê¸° ì¡°ì • */
        font-weight: bold;
    }
    .sidebar-title {
        text-align: center;
        color: #FFA07A; /* Light Salmon: ì—°í•œ ì£¼í™©ìƒ‰ */
        font-size: 2.7em; /* ì‚¬ì´ë“œë°” ì œëª© í¬ê¸° */
        font-weight: bold;
    }
    .sidebar-footer {
        display: flex;
        justify-content: flex-end; /* ì˜¤ë¥¸ìª½ ì •ë ¬ */
        width: 100%;
        padding: 10px 0;
    }
    .sidebar-footer button {
        margin-right: 10px; /* ì˜¤ë¥¸ìª½ ì—¬ë°± ì„¤ì • */
    }
    .stButton > button {
        color: white; /* ê¸€ì ìƒ‰ìƒ */
        background-color: #FF6347; /* Tomato: ë²„íŠ¼ ë°°ê²½ ìƒ‰ìƒ */
        border-radius: 10px; /* ë²„íŠ¼ ëª¨ì„œë¦¬ ë‘¥ê¸€ê²Œ */
        border: none; /* ë²„íŠ¼ í…Œë‘ë¦¬ ì œê±° */
        padding: 10px 20px; /* ë²„íŠ¼ ì—¬ë°± */
        font-size: 16px; /* ë²„íŠ¼ ê¸€ì í¬ê¸° */
        font-weight: bold;
        cursor: pointer;
    }
    .stButton > button:hover {
        background-color: #FF4500; /* Darker Tomato: í˜¸ë²„ ì‹œ ë°°ê²½ ìƒ‰ìƒ */
        color: white; /* í˜¸ë²„ ì‹œ ê¸€ì ìƒ‰ìƒ */
    }
    </style>
    """,
    unsafe_allow_html=True,
)

# ì‚¬ì´ë“œë°” ìƒì„±
with st.sidebar:
    st.markdown("<h2 class='sidebar-title'>ğŸŠ Bitamin Winter Project â˜ƒï¸</h2>", unsafe_allow_html=True)
    st.markdown("<div style='margin-top: 50px;'></div>", unsafe_allow_html=True)

    # ì–¸ì–´ ì„ íƒ
    language_options = {
        "í•œêµ­ì–´": "ko",
        "English": "en-US"
    }
    selected_language = st.selectbox("ì–¸ì–´ ì„ íƒ", list(language_options.keys()), index=list(language_options.values()).index(st.session_state.language))

    # ì–¸ì–´ ë³€ê²½ ì²˜ë¦¬
    if language_options[selected_language] != st.session_state.language:
        st.session_state.language = language_options[selected_language]
        translations = load_translations(lang=st.session_state.language)  # ìƒˆë¡œìš´ ì–¸ì–´ ë¡œë“œ
        config['meta']['language'] = st.session_state.language
        with open('config.toml', 'w', encoding='utf-8') as f:
            toml.dump(config, f)
    st.markdown("<div style='margin-top: 50px;'></div>", unsafe_allow_html=True)
    # íŒŒì¼ ì—…ë¡œë“œ ì„¤ì •
    st.subheader("íŒŒì¼ ì—…ë¡œë“œ ì„¤ì •")
    upload_enabled = st.checkbox("íŒŒì¼ ì—…ë¡œë“œ í™œì„±í™”", value=config['features']['spontaneous_file_upload']['enabled'])

    st.markdown("<div style='margin-top: 50px;'></div>", unsafe_allow_html=True)
    st.markdown("<div style='margin-top: 50px;'></div>", unsafe_allow_html=True)

    # ì•± ì¢…ë£Œ ë²„íŠ¼ ì¶”ê°€
    # ì•± ì¢…ë£Œ ë²„íŠ¼ (ì‚¬ì´ë“œë°” í•˜ë‹¨ ê³ ì •)
    st.markdown('<div class="sidebar-footer">', unsafe_allow_html=True)
    if st.button("ì•± ì¢…ë£Œ"):
        st.stop()
    st.markdown('</div>', unsafe_allow_html=True)

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì œëª© ì„¤ì • (ì¤‘ì•™ ì •ë ¬ ë° ìƒ‰ìƒ ì ìš©)
st.markdown("<h1 class='center-title'>ğŸŠ Bitamin Winter Project â˜ƒï¸</h1>", unsafe_allow_html=True)

# íŒŒì¼ ì—…ë¡œë“œ ìœ„ì ¯
if upload_enabled:
    uploaded_file = st.file_uploader("ë¶„ì„í•  íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (CSV, TXT, JSON ì§€ì›)", type=["csv", "txt", "json"])
    if uploaded_file is not None:
        st.write("ì—…ë¡œë“œëœ íŒŒì¼ì„ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤...")
        try:
            if uploaded_file.name.endswith('.csv'):
                df = pd.read_csv(uploaded_file)
            elif uploaded_file.name.endswith('.json'):
                df = pd.read_json(uploaded_file)
            else:
                df = pd.read_csv(uploaded_file)
            st.dataframe(df)
            st.write("íŒŒì¼ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        except Exception as e:
            st.error("íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

# í˜ì˜¤ í‘œí˜„ ê°ì§€ í…ŒìŠ¤íŠ¸ ë¬¸êµ¬ì™€ UI
if config['features'].get('prompt_playground', False):
    st.markdown("<h2 class='center-text'>í˜ì˜¤ í‘œí˜„ ê°ì§€ í…ŒìŠ¤íŠ¸</h2>", unsafe_allow_html=True)

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ê²°ê³¼ ìƒíƒœ ê´€ë¦¬)
    if "result" not in st.session_state:
        st.session_state.result = None

    # ì…ë ¥ ì°½ ë° ë¶„ì„ ë²„íŠ¼
    # ì…ë ¥ ì°½ ë° ë¶„ì„ ë²„íŠ¼
    if st.session_state.result is None:  # ê²°ê³¼ê°€ ì—†ëŠ” ì´ˆê¸° ìƒíƒœ
        user_input = st.text_area("í˜ì˜¤ í‘œí˜„ì¸ì§€ í™•ì¸í•˜ê³  ì‹¶ì€ ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”:")
        model_path = "./model"  # ëª¨ë¸ì´ ì €ì¥ëœ ê²½ë¡œ
        device = torch.device(
            "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu")

        # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)

        # ë¶„ì„ ë²„íŠ¼
        if st.button("ë¶„ì„í•˜ê¸°"):
            if user_input.strip():  # ì…ë ¥ ë¬¸ì¥ì´ ë¹„ì–´ìˆì§€ ì•Šì„ ê²½ìš°
                # ì…ë ¥ ë¬¸ì¥ í† í°í™”
                inputs = tokenizer(user_input, return_tensors="pt", truncation=True, padding=True).to(device)

                # ëª¨ë¸ ì˜ˆì¸¡
                with torch.no_grad():
                    outputs = model(**inputs)
                    predictions = torch.softmax(outputs.logits, dim=1)
                    confidence, label = torch.max(predictions, dim=1)  # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ë¼ë²¨

                # ê²°ê³¼ ì €ì¥
                if label.item() == 1:  # 1: í˜ì˜¤ í‘œí˜„ìœ¼ë¡œ ë¶„ë¥˜ëœ ê²½ìš°
                    st.session_state.result = f"ì…ë ¥í•œ ë¬¸ì¥ \"{user_input}\" ì€ í˜ì˜¤ í‘œí˜„ìœ¼ë¡œ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."
                else:  # 0: í˜ì˜¤ í‘œí˜„ì´ ì•„ë‹Œ ê²½ìš°
                    st.session_state.result = f"ì…ë ¥í•œ ë¬¸ì¥ \"{user_input}\" ì€ í˜ì˜¤ í‘œí˜„ì´ ì•„ë‹™ë‹ˆë‹¤."
            else:
                st.warning("ë¬¸ì¥ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        # ê²°ê³¼ í‘œì‹œ
        st.write(st.session_state.result)

        # ìƒˆë¡œìš´ ì…ë ¥ ë²„íŠ¼
        if st.button("ìƒˆë¡œìš´ ì…ë ¥"):
            st.session_state.result = None  # ê²°ê³¼ ì´ˆê¸°í™”

